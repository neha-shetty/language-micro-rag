# ğŸ“„ Takeâ€‘Home Assessment: Microâ€‘RAG Prototype

---

## ğŸ¯ **Assessment Overview**

**Task:**  
The assessment required building a **Micro-RAG prototype** to demonstrate an endâ€‘toâ€‘end Retrievalâ€‘Augmented Generation workflow within **2â€“4 hours**.  

**Requirements included:**
- âœ… Build a tiny dataset (â‰¥10 short Markdown or plainâ€‘text FAQ files, each â‰¤1k tokens).
- âœ… Add a `questions.json` file with 3â€“5+ answerable questions.
- âœ… Write `ingest.py`: load the files, chunk them, embed them, and store them in a vector store.
- âœ… Write `answer.py` or a notebook: load the index, retrieve relevant chunks, run an LLM call, and return answers with file-level citations.
- âœ… Include a short `README` explaining setup, run commands, chunking choices, and tech rationale.

---

## ğŸ“Œ **What I built**

**âœ… Tiny dataset**  
I created `sample_data/` with 12 Markdown FAQ files covering basic French, German, and Italian phrases. This simulates a real multilingual phrasebook. The folder includes `questions.json` with 10 realistic questions (e.g. â€œHow do I say hello in German?â€).

**âœ… `ingest.py`**  
This script:
- Loads the FAQ files.
- Splits them into small, overlapping text chunks (~500 characters) to balance context with retrieval accuracy.
- Generates embeddings using a local Hugging Face `sentence-transformers` model (free to run).
- Stores all chunk embeddings in a local **FAISS** index for fast similarity search.

**âœ… `answer.py`**  
This script:
- Loads the FAISS index.
- For each question in `questions.json`, runs a similarity search to find the most relevant chunks.
- Builds a context prompt.
- Sends the prompt to a **local LLM** running through **Ollama** (Llama3) to generate a natural answer.
- Outputs JSON with the answer and the source files used.

---

##  **Why these tech choices**

- **No paid APIs:** I used Hugging Face local embeddings + Ollama to run everything fully locally, with no OpenAI or cloud cost.
- **FAISS:** Simple, fast local vector store- perfect for local RAG prototyping.
- **Chunking:** Small overlapping chunks keep answers relevant but preserve context for short queries.
- **Ollama:** Runs a modern openâ€‘source Llama3 model locally to generate answers, so the flow works offline.

---
